{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/input/ptbxl-all-diag\") # Перейдем в Input (только для чтения!)\n!ls # Посмотреть содержимое","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:36:15.206767Z","iopub.execute_input":"2023-07-03T11:36:15.207902Z","iopub.status.idle":"2023-07-03T11:36:16.220468Z","shell.execute_reply.started":"2023-07-03T11:36:15.207867Z","shell.execute_reply":"2023-07-03T11:36:16.218906Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"X_train_ptbxl_all.npy\tX_val_ptbxl_diag.npy\ty_val_ptbxl_all.npy\nX_train_ptbxl_diag.npy\ty_train_ptbxl_all.npy\ty_val_ptbxl_diag.npy\nX_val_ptbxl_all.npy\ty_train_ptbxl_diag.npy\n","output_type":"stream"}]},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/\") # Перейдем в Output\n!ls","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:36:16.223047Z","iopub.execute_input":"2023-07-03T11:36:16.223435Z","iopub.status.idle":"2023-07-03T11:36:17.408325Z","shell.execute_reply.started":"2023-07-03T11:36:16.223398Z","shell.execute_reply":"2023-07-03T11:36:17.407116Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"__notebook_source__.ipynb\n","output_type":"stream"}]},{"cell_type":"code","source":"# Для работы с данными\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt   # plotting\nimport seaborn as sns   # plotting heatmap\nimport tensorflow as tf\nimport math\nimport pickle\n\n# Для работы с моделями\nfrom tensorflow import keras\nfrom keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import load_model, save_model\n\n# Для метрик\nfrom keras import backend as K\nfrom keras.metrics import AUC, Recall, Precision, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\nfrom sklearn.metrics import fbeta_score, precision_score, recall_score, accuracy_score, roc_auc_score\nfrom sklearn.metrics import auc, roc_curve, classification_report\nfrom sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, f1_score\n\n# Код из учебника Жерона\nK = keras.backend\n\n# # 71 класс SCP_ECG в алфавитном порядке для категории 'all'\nlabels = ['1AVB', '2AVB', '3AVB', 'ABQRS', 'AFIB', 'AFLT', \n          'ALMI', 'AMI', 'ANEUR', 'ASMI', 'BIGU', 'CLBBB', \n          'CRBBB', 'DIG', 'EL', 'HVOLT', 'ILBBB', 'ILMI', \n          'IMI', 'INJAL', 'INJAS', 'INJIL', 'INJIN','INJLA', \n          'INVT', 'IPLMI', 'IPMI', 'IRBBB', 'ISC_', 'ISCAL', \n          'ISCAN', 'ISCAS', 'ISCIL', 'ISCIN', 'ISCLA', 'IVCD', \n          'LAFB', 'LAO/LAE', 'LMI', 'LNGQT', 'LOWT', 'LPFB', \n          'LPR', 'LVH', 'LVOLT', 'NDT', 'NORM', 'NST_', \n          'NT_', 'PAC', 'PACE', 'PMI', 'PRC(S)', 'PSVT', \n          'PVC', 'QWAVE', 'RAO/RAE', 'RVH', 'SARRH', 'SBRAD', \n          'SEHYP', 'SR', 'STACH', 'STD_', 'STE_', 'SVARR', \n          'SVTAC', 'TAB_', 'TRIGU', 'VCLVH', 'WPW']\n\n# 44 класса SCP_ECG в алфавитном порядке для категории 'diag'\n# labels = ['1AVB', '2AVB', '3AVB',\n#           'ALMI', 'AMI', 'ANEUR', 'ASMI', 'CLBBB', \n#           'CRBBB', 'DIG', 'EL', 'ILBBB', 'ILMI', \n#           'IMI', 'INJAL', 'INJAS', 'INJIL', 'INJIN','INJLA', \n#           'IPLMI', 'IPMI', 'IRBBB', 'ISC_', 'ISCAL', \n#           'ISCAN', 'ISCAS', 'ISCIL', 'ISCIN', 'ISCLA', 'IVCD', \n#           'LAFB', 'LAO/LAE', 'LMI', 'LNGQT', 'LPFB', \n#           'LVH', 'NDT', 'NORM', 'NST_', \n#           'PMI', \n#           'RAO/RAE', 'RVH', \n#           'SEHYP', 'WPW']\n\n\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_batch_end(self, batch, logs):\n        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n        self.losses.append(logs[\"loss\"])\n        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n\ndef find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n    init_weights = model.get_weights()\n    iterations = math.ceil(len(X) / batch_size) * epochs\n    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n    init_lr = K.get_value(model.optimizer.learning_rate)\n    K.set_value(model.optimizer.learning_rate, min_rate)\n    exp_lr = ExponentialLearningRate(factor)\n    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n                        callbacks=[exp_lr])\n    K.set_value(model.optimizer.learning_rate, init_lr)\n    model.set_weights(init_weights)\n    return exp_lr.rates, exp_lr.losses\n\ndef plot_lr_vs_loss(rates, losses):\n    plt.plot(rates, losses)\n    plt.gca().set_xscale('log')\n    plt.hlines(min(losses), min(rates), max(rates))\n    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n    plt.xlabel(\"Learning rate\")\n    plt.ylabel(\"Loss\")\n\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_epoch_begin(self, epoch, logs=None):\n        self.prev_loss = 0\n    def on_batch_end(self, batch, logs=None):\n        batch_loss = logs[\"loss\"] * (batch + 1) - self.prev_loss * batch\n        self.prev_loss = logs[\"loss\"]\n        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n        self.losses.append(batch_loss)\n        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n\nclass OneCycleScheduler(keras.callbacks.Callback):\n    def __init__(self, iterations, max_rate, start_rate=None,\n                 last_iterations=None, last_rate=None):\n        self.iterations = iterations\n        self.max_rate = max_rate\n        self.start_rate = start_rate or max_rate / 10\n        self.last_iterations = last_iterations or iterations // 10 + 1\n        self.half_iteration = (iterations - self.last_iterations) // 2\n        self.last_rate = last_rate or self.start_rate / 1000\n        self.iteration = 0\n    def _interpolate(self, iter1, iter2, rate1, rate2):\n        return ((rate2 - rate1) * (self.iteration - iter1)\n                / (iter2 - iter1) + rate1)\n    def on_batch_begin(self, batch, logs):\n        if self.iteration < self.half_iteration:\n            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n        elif self.iteration < 2 * self.half_iteration:\n            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n                                     self.max_rate, self.start_rate)\n        else:\n            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n                                     self.start_rate, self.last_rate)\n        self.iteration += 1\n        K.set_value(self.model.optimizer.learning_rate, rate)\n\nclass attention(layers.Layer):\n    def __init__(self,**kwargs):\n        super(attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1),\n                               initializer='random_normal', trainable=True)\n        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1),\n                               initializer='zeros', trainable=True)\n        super(attention, self).build(input_shape)\n\n    def call(self,x):\n        # Alignment scores. Pass them through tanh function\n        e = K.tanh(K.dot(x,self.W)+self.b)\n        # Remove dimension of size 1\n        e = K.squeeze(e, axis=-1)\n        # Compute the weights\n        alpha = K.softmax(e)\n        # Reshape to tensorFlow format\n        alpha = K.expand_dims(alpha, axis=-1)\n        # Compute the context vector\n        context = x * alpha\n        context = K.sum(context, axis=1)\n        return context\n\ndef ATI_CNN(num_classes, multi_label_classifier=False):\n    activation = keras.activations.sigmoid if multi_label_classifier else keras.activations.softmax\n\n    def Conv_block_1(filters):\n        return keras.Sequential([\n            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.MaxPooling1D(pool_size=3, strides=3),\n            layers.Dropout(rate=0.2)\n        ])\n\n    def Conv_block_2(filters):\n        return keras.Sequential([\n            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.MaxPooling1D(pool_size=3, strides=3),\n            layers.Dropout(rate=0.2)\n        ])\n\n    shape = (1000, 12)\n    input = keras.Input(shape=shape, name=\"ECG\")\n\n    x = Conv_block_1(64)(input)\n    x = Conv_block_1(128)(x)\n\n    x = Conv_block_2(256)(x)\n    x = Conv_block_2(256)(x)\n    x = Conv_block_2(256)(x)\n\n    x = layers.LSTM(units=32, return_sequences=True)(x)\n    x = layers.LSTM(units=32, return_sequences=True)(x)\n\n    x = attention()(x)\n\n    output = layers.Dense(units=num_classes, activation=activation, name='output')(x)\n\n    model = keras.Model(inputs=[input], outputs=[output])\n\n    return model\n\ndef M_ATI_CNN(num_classes, multi_label_classifier=False):\n    activation = layers.Activation(keras.activations.sigmoid) if multi_label_classifier else layers.Activation(keras.activations.softmax)\n\n    def Conv_block_1(filters):\n        return keras.Sequential([\n            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.MaxPooling1D(pool_size=3, strides=3),\n            layers.Dropout(rate=0.2)\n        ])\n\n    def Conv_block_2(filters):\n        return keras.Sequential([\n            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.MaxPooling1D(pool_size=3, strides=3),\n            layers.Dropout(rate=0.2)\n        ])\n\n    shape = (1000, 12)\n    input = keras.Input(shape=shape, name=\"ECG\")\n\n    output_modules = list()\n\n    for i in range(num_classes):\n      output_modules.append(keras.Sequential([\n          Conv_block_1(64),\n          Conv_block_1(128),\n          Conv_block_2(256),\n          Conv_block_2(256),\n          Conv_block_2(256)\n      ])(input))\n\n      output_modules[-1] = layers.Bidirectional(layers.LSTM(units=32, return_sequences=True))(output_modules[-1])\n\n      output_modules[-1] = attention()(output_modules[-1])\n      output_modules[-1] = layers.Dense(units=1)(output_modules[-1])\n\n    cat_outputs =  keras.layers.Concatenate()(output_modules)\n\n    output = activation(cat_outputs)\n\n    model = keras.Model(inputs=[input], outputs=[output])\n\n    return model\n\ndef DNN(num_classes, multi_label_classifier=False):\n    activation = keras.activations.sigmoid if multi_label_classifier else keras.activations.softmax\n\n    def Conv_block_1(filters, kernel_size, pool_size):\n        return keras.Sequential([\n            layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same'),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.Dropout(rate=0.1),\n            layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same'),\n            layers.MaxPooling1D(pool_size=pool_size)\n        ]), layers.MaxPooling1D(pool_size=pool_size)\n\n    def Conv_block_2(filters, kernel_size, pool_size):\n        return keras.Sequential([\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.Dropout(rate=0.1),\n            layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same'),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.Dropout(rate=0.1),\n            layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same'),\n            layers.MaxPooling1D(pool_size=pool_size)\n        ]), layers.MaxPooling1D(pool_size=pool_size)\n\n    shape = (1000, 12)\n    input = keras.Input(shape=shape, name=\"ECG\")\n\n    x = keras.Sequential([\n        layers.Conv1D(filters=32, kernel_size=16, padding='same'),\n        layers.BatchNormalization(),\n        layers.ReLU()\n    ])(input)\n\n    x1, x2 = Conv_block_1(kernel_size=16, filters=32, pool_size=2)\n    x = layers.Concatenate()([x1(x), x2(x)])\n\n    for filters, kernel_size, pool_size in zip(\n        [32, 32, 32, 64, 64, 64, 64, 96, 96, 96, 96],\n        [16, 16, 16, 16, 16,  8,  8,  8,  8,  8,  8],\n        [ 1,  2,  1,  2,  1,  2,  1,  2,  1,  2,  1]\n    ):\n        x1, x2 = Conv_block_2(filters=filters, kernel_size=kernel_size, pool_size=pool_size)\n        x = layers.Concatenate()([x1(x), x2(x)])\n\n    x = keras.Sequential([\n        layers.BatchNormalization(),\n        layers.ReLU()\n    ])(x)\n\n    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True), merge_mode='sum')(x)\n    x = layers.GlobalMaxPooling1D()(x)\n\n    x = layers.Dense(units=64, activation=keras.activations.relu)(x)\n    output = layers.Dense(units=num_classes, activation=activation, name='output')(x)\n\n    model = keras.Model(inputs=[input], outputs=[output])\n\n    return model\n\n# Загрузка ptbxl\ndef load_ptbxl(task):\n  os.chdir(\"/kaggle/input/ptbxl-all-diag\")\n  if task == 'diag':\n    X_train = np.load('X_train_ptbxl_diag.npy')\n    y_train = np.load('y_train_ptbxl_diag.npy')\n    X_test = np.load('X_val_ptbxl_diag.npy')\n    y_test = np.load('y_val_ptbxl_diag.npy')\n  elif task == 'all':\n    X_train = np.load('X_train_ptbxl_all.npy')\n    y_train = np.load('y_train_ptbxl_all.npy')\n    X_test = np.load('X_val_ptbxl_all.npy')\n    y_test = np.load('y_val_ptbxl_all.npy')\n  os.chdir(\"/kaggle/working/\")\n  return X_train, y_train, X_test, y_test\n\n# Сериализация\ndef sp(obj, name): # sp = serialization_pickle ;name = *.pkl\n    with open(name, 'wb') as pickle_out:\n        pickle.dump(obj, pickle_out)\n\n# Компиляция и обучение модели\ndef AUC_Keras(y_true, y_pred):\n    auc = keras.metrics.AUC(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc\n\n# Компиляция и обучение модели\ndef compile_fit(model, X_train, y_train, X_val = None, y_val = None, validation_split = 0.0, batch_size = None, epochs = None, early_stopping = None, model_checkpoint = None, onecycle = None):\n  model.compile(loss = keras.losses.BinaryCrossentropy(),\n                optimizer=tf.keras.optimizers.experimental.AdamW(),\n                metrics=['AUC'])\n\n  if X_val == None:\n    history = model.fit(X_train, y_train,\n                        epochs = epochs,\n                        batch_size = batch_size,\n                        validation_data = None,\n                        validation_split=validation_split,\n                        callbacks=[onecycle, model_checkpoint, early_stopping])\n    sp(history, 'history.pkl')\n  else:\n    history = model.fit(X_train, y_train,\n                        epochs = epochs,\n                        batch_size = batch_size,\n                        validation_data = (X_val, y_val),\n                        validation_split=0.0,\n                        callbacks=[onecycle, model_checkpoint, early_stopping])\n    sp(history, 'history.pkl')\n  return history\n\n# График loss и accuracy\ndef plot_loss_and_accuracy_curves(_history):\n  fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18,6))\n  axs[0].plot(_history.history['loss'], color='b', label='Training loss')\n  axs[0].plot(_history.history['val_loss'], color='r', label='Validation loss')\n  axs[0].set_title(\"Loss\")\n  axs[0].legend(loc='best', shadow=True)\n  axs[1].plot(_history.history['auc'], color='b', label='Training accuracy')\n  axs[1].plot(_history.history['val_auc'], color='r', label='Validation accuracy')\n  axs[1].set_title(\"AUC\")\n  axs[1].legend(loc='best', shadow=True)\n  plt.show()\n\n# Работа с моделями lstm и lstm_bidir\ndef type_comp_fit_save_model_score(X_train, y_train, X_test, y_test, type_model, save_name, index_model_task, batch_size, epochs, onecycle):\n  # количество классов\n  num_classes = y_train.shape[1]\n\n  # Выбор архитектуры модели\n  if type_model == 'lstm':\n    inputs = keras.Input(shape=(1000, 12))\n    x = layers.LSTM(units=256,\n                    return_sequences=True,\n                    stateful=False,\n                    unroll=False)(inputs)\n    x = layers.LeakyReLU()(x)\n    x = layers.LSTM(units=256,\n                    return_sequences=True, # True if concat\n                    stateful=False,\n                    unroll=False)(x)\n    x = layers.LeakyReLU()(x)\n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    concat = layers.Concatenate()([avg_pool, max_pool])\n    outputs = layers.Dense(units=num_classes, activation='sigmoid')(concat)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    print(model.summary())\n\n    # Реализация раннего прекращения.\n    checkpoint_filepath = './checkpoint_lstm/'\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n                                                          save_weights_only=True,\n                                                          save_best_only=True)\n    early_stopping = keras.callbacks.EarlyStopping(patience=epochs//10,\n                                                  restore_best_weights=True)\n  elif type_model == 'lstm_bidir':\n    inputs = keras.Input(shape=(1000, 12))\n    x = layers.Bidirectional(layers.LSTM(units=256,\n                             return_sequences = True,\n                             stateful = False,\n                             unroll = False))(inputs)\n    x = layers.LeakyReLU()(x)\n    x = layers.Bidirectional(layers.LSTM(units=256,\n                             return_sequences = True,\n                             stateful = False,\n                             unroll = False))(x)\n    x = layers.LeakyReLU()(x)\n    avg_pool = layers.GlobalAveragePooling1D()(x)\n    max_pool = layers.GlobalMaxPooling1D()(x)\n    concat = layers.Concatenate()([avg_pool, max_pool])\n    outputs = layers.Dense(units=num_classes, activation='sigmoid')(concat)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n\n    model.build(input_shape = (None, 1000, 12)) # `input_shape` is the shape of the input data\n\n    print(model.summary())\n\n    # Реализация раннего прекращения.\n    checkpoint_filepath = './checkpoint_lstm_bidir/'\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n                                                          save_weights_only=True,\n                                                          save_best_only=True)\n    early_stopping = keras.callbacks.EarlyStopping(patience=epochs//10,\n                                                  restore_best_weights=True)\n\n  # Обучение\n  History = compile_fit(model, X_train, y_train, validation_split=0.1, batch_size=batch_size, epochs=epochs, early_stopping=early_stopping, model_checkpoint=model_checkpoint, onecycle=onecycle)\n\n  # Сохранение модели\n  model.save(save_name)\n\n  # Построение графика\n  plot_loss_and_accuracy_curves(History)\n\n  return model\n\n# Подсчет метрик f1, f2, g1 и g2 (где 1 и 2 значения параметра beta) для каждого класса в задаче многозначной классификации (multilabel classication)\ndef calc_metrics(y_true, y_pred, thresholds, labels): \n  num_classes = y_true.shape[1] # n x m, где m - количество классов (применялось one-hot кодирование)\n  table_scores = pd.DataFrame({'label': labels[:num_classes], \n                               'f1': pd.Series([]), \n                               'f2': pd.Series([]), \n                               'g1': pd.Series([]), \n                               'g2': pd.Series([])}, index = range(1, num_classes + 1))\n    \n  for i, conf_matr in enumerate(multilabel_confusion_matrix(y_true, y_pred > thresholds)):\n    \n    label_index = i + 1 # i + 1, потому что в table_scores индексация начинается с 1                            \n    true = y_true[:, i]\n    pred = y_pred[:, i]\n    table_scores.loc[label_index, 'f1'] = fbeta_score(true, pred > thresholds[i], beta = 1) \n    table_scores.loc[label_index, 'f2'] = fbeta_score(true, pred > thresholds[i], beta = 2)\n                                \n    TP = conf_matr[1, 1]\n    FP = conf_matr[0, 1]\n    TN = conf_matr[0, 0]\n    FN = conf_matr[1, 0]                        \n    beta = 1\n    g1 = TP/(TP + FP + beta * FN) # beta = 1\n    beta = 2\n    g2 = TP/(TP + FP + beta * FN) # beta = 2\n    table_scores.loc[label_index, 'g1'] = g1\n    table_scores.loc[label_index, 'g2'] = g2\n                                \n  return table_scores\n\ndef plot_roc_and_cm_for_multi_label_clf(y_test, y_pred, labels):\n  fig, axes = plt.subplots(12, 6, figsize=(100, 100))\n  model_thresholds = list()\n\n  for i, label in enumerate(labels):\n    fpr, tpr, thresholds = roc_curve(y_test[:, i], y_pred[:, i])\n    roc_auc = auc(fpr, tpr)\n\n    idx = (tpr - fpr + 1).argmax()\n    model_thresholds.append(thresholds[idx])\n\n    ax = axes[i // 6, i % 6]\n\n    ax.plot(fpr, tpr, color='darkorange',\n            label=f'{label} vs the rest (area = {roc_auc:0.2f})', zorder=0)\n    ax.plot([0, 1], [0, 1], color='k', linestyle='--', label=\"chance level (AUC = 0.5)\")\n    ax.scatter([fpr[idx]], [tpr[idx]], color='g', marker='x', label='optimal value', zorder=1)\n    ax.set(xlim=[0.0, 1.0], ylim=[0.0, 1.05],\n          xlabel='False Positive Rate',\n          ylabel='True Positive Rate',\n          title=f'{label}',)\n    ax.legend(loc=\"lower right\")\n\n  fig.delaxes(axes[-1, -1])\n  fig.suptitle('lstm_bidir ROC кривые', fontsize=20)\n  fig.savefig('/lstm_bidir/lstm_bidir_ROC_curves.png')\n\n  model_thresholds = np.array(model_thresholds)\n\n  fig, axes = plt.subplots(12, 6, figsize=(100, 100))\n\n  for i, (conf_matr, label) in enumerate(zip(multilabel_confusion_matrix(y_test, y_pred > model_thresholds), labels)):\n    ax = axes[i // 6, i % 6]\n    sns.heatmap(conf_matr,\n              square=True,\n              annot=True,\n              fmt='.0f',\n              cmap=plt.cm.Blues,\n              ax=ax)\n    ax.set_xticklabels([f'not {label}', label])\n    ax.set_yticklabels([f'not {label}', label], rotation=45, ha='right')\n    ax.set(xlabel=\"Predicted label\", ylabel=\"True label\")\n\n  fig.delaxes(axes[-1, -1])\n  fig.suptitle('lstm bidir confusion matrix', fontsize=20)\n\n  fig.savefig('/lstm_bidir/lstm_bidir_cm.png')\n\n  print(classification_report(y_test, y_pred > model_thresholds, target_names=labels))\n  pass\n  return model_thresholds\n\ndef experiment(X_train, y_train, X_test, y_test, batch_size, epochs, type_model, save, index_model_task, labels):\n  onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * epochs, max_rate=0.0001)\n  model = type_comp_fit_save_model_score(X_train, y_train, X_test, y_test, type_model = type_model, save_name = save, index_model_task = index_model_task, batch_size=batch_size, epochs=epochs, onecycle=onecycle)\n  print(model.evaluate(X_test, y_test))\n  y_pred = model.predict(X_test)\n  model_thresholds = plot_roc_and_cm_for_multi_label_clf(y_test, y_pred, labels)\n  table = calc_metrics(y_test, y_pred, model_thresholds, labels)\n  sp(table, 'table.pkl')\n  return model, table\n\ntf.random.set_seed(42)\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-03T11:36:17.411001Z","iopub.execute_input":"2023-07-03T11:36:17.416066Z","iopub.status.idle":"2023-07-03T11:36:35.579701Z","shell.execute_reply.started":"2023-07-03T11:36:17.416028Z","shell.execute_reply":"2023-07-03T11:36:35.578692Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Модель lstm_bidir на категории 'all'","metadata":{}},{"cell_type":"code","source":"X_train, y_train, X_test, y_test = load_ptbxl(task = 'all')\nsave = 'lstm_bidir/model_lstm_bidir_all'\nlstm_bidir, table = experiment(X_train, y_train, X_test, y_test, 256, 400, 'lstm_bidir', save, 'lstm_bidir_all', labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:36:43.829507Z","iopub.execute_input":"2023-07-03T11:36:43.830255Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 1000, 12)]   0           []                               \n                                                                                                  \n bidirectional (Bidirectional)  (None, 1000, 512)    550912      ['input_1[0][0]']                \n                                                                                                  \n leaky_re_lu (LeakyReLU)        (None, 1000, 512)    0           ['bidirectional[0][0]']          \n                                                                                                  \n bidirectional_1 (Bidirectional  (None, 1000, 512)   1574912     ['leaky_re_lu[0][0]']            \n )                                                                                                \n                                                                                                  \n leaky_re_lu_1 (LeakyReLU)      (None, 1000, 512)    0           ['bidirectional_1[0][0]']        \n                                                                                                  \n global_average_pooling1d (Glob  (None, 512)         0           ['leaky_re_lu_1[0][0]']          \n alAveragePooling1D)                                                                              \n                                                                                                  \n global_max_pooling1d (GlobalMa  (None, 512)         0           ['leaky_re_lu_1[0][0]']          \n xPooling1D)                                                                                      \n                                                                                                  \n concatenate (Concatenate)      (None, 1024)         0           ['global_average_pooling1d[0][0]'\n                                                                 , 'global_max_pooling1d[0][0]']  \n                                                                                                  \n dense (Dense)                  (None, 71)           72775       ['concatenate[0][0]']            \n                                                                                                  \n==================================================================================================\nTotal params: 2,198,599\nTrainable params: 2,198,599\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nEpoch 1/400\n70/70 [==============================] - 107s 1s/step - loss: 0.6624 - auc: 0.5466 - val_loss: 0.6405 - val_auc: 0.6321\nEpoch 2/400\n70/70 [==============================] - 95s 1s/step - loss: 0.5935 - auc: 0.6560 - val_loss: 0.5573 - val_auc: 0.6709\nEpoch 3/400\n70/70 [==============================] - 97s 1s/step - loss: 0.4243 - auc: 0.7066 - val_loss: 0.2974 - val_auc: 0.7283\nEpoch 4/400\n70/70 [==============================] - 97s 1s/step - loss: 0.2083 - auc: 0.7458 - val_loss: 0.1718 - val_auc: 0.7770\nEpoch 5/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1442 - auc: 0.7960 - val_loss: 0.1345 - val_auc: 0.8525\nEpoch 6/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1233 - auc: 0.8416 - val_loss: 0.1222 - val_auc: 0.8791\nEpoch 7/400\n70/70 [==============================] - 96s 1s/step - loss: 0.1158 - auc: 0.8614 - val_loss: 0.1174 - val_auc: 0.8857\nEpoch 8/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1125 - auc: 0.8734 - val_loss: 0.1152 - val_auc: 0.8887\nEpoch 9/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1107 - auc: 0.8799 - val_loss: 0.1140 - val_auc: 0.8913\nEpoch 10/400\n70/70 [==============================] - 96s 1s/step - loss: 0.1096 - auc: 0.8844 - val_loss: 0.1132 - val_auc: 0.8923\nEpoch 11/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1088 - auc: 0.8864 - val_loss: 0.1125 - val_auc: 0.8915\nEpoch 12/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1082 - auc: 0.8879 - val_loss: 0.1122 - val_auc: 0.8913\nEpoch 13/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1077 - auc: 0.8887 - val_loss: 0.1116 - val_auc: 0.8919\nEpoch 14/400\n70/70 [==============================] - 96s 1s/step - loss: 0.1074 - auc: 0.8893 - val_loss: 0.1112 - val_auc: 0.8923\nEpoch 15/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1070 - auc: 0.8899 - val_loss: 0.1109 - val_auc: 0.8931\nEpoch 16/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1067 - auc: 0.8908 - val_loss: 0.1108 - val_auc: 0.8943\nEpoch 17/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1063 - auc: 0.8920 - val_loss: 0.1105 - val_auc: 0.8949\nEpoch 18/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1060 - auc: 0.8928 - val_loss: 0.1101 - val_auc: 0.8963\nEpoch 19/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1058 - auc: 0.8933 - val_loss: 0.1095 - val_auc: 0.8967\nEpoch 20/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1057 - auc: 0.8940 - val_loss: 0.1092 - val_auc: 0.8981\nEpoch 21/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1051 - auc: 0.8954 - val_loss: 0.1085 - val_auc: 0.8992\nEpoch 22/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1055 - auc: 0.8942 - val_loss: 0.1096 - val_auc: 0.8963\nEpoch 23/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1053 - auc: 0.8943 - val_loss: 0.1087 - val_auc: 0.8983\nEpoch 24/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1047 - auc: 0.8957 - val_loss: 0.1076 - val_auc: 0.8989\nEpoch 25/400\n70/70 [==============================] - 96s 1s/step - loss: 0.1042 - auc: 0.8967 - val_loss: 0.1075 - val_auc: 0.8996\nEpoch 26/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1041 - auc: 0.8973 - val_loss: 0.1076 - val_auc: 0.9001\nEpoch 27/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1034 - auc: 0.8989 - val_loss: 0.1069 - val_auc: 0.9014\nEpoch 28/400\n70/70 [==============================] - 96s 1s/step - loss: 0.1030 - auc: 0.9000 - val_loss: 0.1069 - val_auc: 0.9015\nEpoch 29/400\n70/70 [==============================] - 97s 1s/step - loss: 0.1021 - auc: 0.9013 - val_loss: 0.1054 - val_auc: 0.9031\nEpoch 30/400\n70/70 [==============================] - 98s 1s/step - loss: 0.1013 - auc: 0.9031 - val_loss: 0.1044 - val_auc: 0.9046\nEpoch 31/400\n70/70 [==============================] - 96s 1s/step - loss: 0.1006 - auc: 0.9044 - val_loss: 0.1037 - val_auc: 0.9058\nEpoch 32/400\n70/70 [==============================] - 96s 1s/step - loss: 0.1006 - auc: 0.9045 - val_loss: 0.1032 - val_auc: 0.9070\nEpoch 33/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0996 - auc: 0.9067 - val_loss: 0.1026 - val_auc: 0.9079\nEpoch 34/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0991 - auc: 0.9079 - val_loss: 0.1030 - val_auc: 0.9072\nEpoch 35/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0997 - auc: 0.9073 - val_loss: 0.1027 - val_auc: 0.9083\nEpoch 36/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0983 - auc: 0.9101 - val_loss: 0.1012 - val_auc: 0.9108\nEpoch 37/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0975 - auc: 0.9120 - val_loss: 0.1009 - val_auc: 0.9110\nEpoch 38/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0971 - auc: 0.9134 - val_loss: 0.1017 - val_auc: 0.9101\nEpoch 39/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0970 - auc: 0.9136 - val_loss: 0.0999 - val_auc: 0.9135\nEpoch 40/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0961 - auc: 0.9156 - val_loss: 0.0994 - val_auc: 0.9146\nEpoch 41/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0957 - auc: 0.9170 - val_loss: 0.0986 - val_auc: 0.9162\nEpoch 42/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0957 - auc: 0.9169 - val_loss: 0.0998 - val_auc: 0.9140\nEpoch 43/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0951 - auc: 0.9184 - val_loss: 0.0978 - val_auc: 0.9182\nEpoch 44/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0944 - auc: 0.9199 - val_loss: 0.0976 - val_auc: 0.9184\nEpoch 45/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0952 - auc: 0.9189 - val_loss: 0.0993 - val_auc: 0.9167\nEpoch 46/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0959 - auc: 0.9164 - val_loss: 0.1026 - val_auc: 0.9135\nEpoch 47/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0972 - auc: 0.9141 - val_loss: 0.1000 - val_auc: 0.9133\nEpoch 48/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0968 - auc: 0.9145 - val_loss: 0.0987 - val_auc: 0.9164\nEpoch 49/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0963 - auc: 0.9166 - val_loss: 0.0986 - val_auc: 0.9190\nEpoch 50/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0948 - auc: 0.9188 - val_loss: 0.0967 - val_auc: 0.9203\nEpoch 51/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0935 - auc: 0.9215 - val_loss: 0.0961 - val_auc: 0.9208\nEpoch 52/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0926 - auc: 0.9240 - val_loss: 0.0951 - val_auc: 0.9252\nEpoch 53/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0921 - auc: 0.9251 - val_loss: 0.0945 - val_auc: 0.9249\nEpoch 54/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0919 - auc: 0.9250 - val_loss: 0.0946 - val_auc: 0.9245\nEpoch 55/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0917 - auc: 0.9261 - val_loss: 0.0943 - val_auc: 0.9259\nEpoch 56/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0912 - auc: 0.9272 - val_loss: 0.0932 - val_auc: 0.9280\nEpoch 57/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0909 - auc: 0.9273 - val_loss: 0.0927 - val_auc: 0.9295\nEpoch 58/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0905 - auc: 0.9293 - val_loss: 0.0940 - val_auc: 0.9281\nEpoch 59/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0921 - auc: 0.9242 - val_loss: 0.0980 - val_auc: 0.9161\nEpoch 60/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0917 - auc: 0.9264 - val_loss: 0.0925 - val_auc: 0.9311\nEpoch 61/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0898 - auc: 0.9308 - val_loss: 0.0915 - val_auc: 0.9324\nEpoch 62/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0889 - auc: 0.9327 - val_loss: 0.0909 - val_auc: 0.9335\nEpoch 63/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0882 - auc: 0.9340 - val_loss: 0.0906 - val_auc: 0.9345\nEpoch 64/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0878 - auc: 0.9349 - val_loss: 0.0898 - val_auc: 0.9351\nEpoch 65/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0875 - auc: 0.9358 - val_loss: 0.0894 - val_auc: 0.9361\nEpoch 66/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0866 - auc: 0.9371 - val_loss: 0.0892 - val_auc: 0.9364\nEpoch 67/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0862 - auc: 0.9380 - val_loss: 0.0886 - val_auc: 0.9366\nEpoch 68/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0857 - auc: 0.9386 - val_loss: 0.0878 - val_auc: 0.9382\nEpoch 69/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0851 - auc: 0.9399 - val_loss: 0.0882 - val_auc: 0.9380\nEpoch 70/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0851 - auc: 0.9401 - val_loss: 0.0872 - val_auc: 0.9393\nEpoch 71/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0855 - auc: 0.9391 - val_loss: 0.0875 - val_auc: 0.9399\nEpoch 72/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0847 - auc: 0.9409 - val_loss: 0.0869 - val_auc: 0.9399\nEpoch 73/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0855 - auc: 0.9393 - val_loss: 0.0875 - val_auc: 0.9402\nEpoch 74/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0847 - auc: 0.9411 - val_loss: 0.0902 - val_auc: 0.9350\nEpoch 75/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0845 - auc: 0.9412 - val_loss: 0.0869 - val_auc: 0.9398\nEpoch 76/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0841 - auc: 0.9421 - val_loss: 0.0858 - val_auc: 0.9426\nEpoch 77/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0831 - auc: 0.9437 - val_loss: 0.0854 - val_auc: 0.9431\nEpoch 78/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0828 - auc: 0.9443 - val_loss: 0.0849 - val_auc: 0.9437\nEpoch 79/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0825 - auc: 0.9447 - val_loss: 0.0846 - val_auc: 0.9454\nEpoch 80/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0818 - auc: 0.9460 - val_loss: 0.0842 - val_auc: 0.9446\nEpoch 81/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0817 - auc: 0.9462 - val_loss: 0.0835 - val_auc: 0.9457\nEpoch 82/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0810 - auc: 0.9471 - val_loss: 0.0835 - val_auc: 0.9457\nEpoch 83/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0818 - auc: 0.9443 - val_loss: 0.1009 - val_auc: 0.9199\nEpoch 84/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0853 - auc: 0.9386 - val_loss: 0.0852 - val_auc: 0.9434\nEpoch 85/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0815 - auc: 0.9463 - val_loss: 0.0831 - val_auc: 0.9459\nEpoch 86/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0804 - auc: 0.9479 - val_loss: 0.0825 - val_auc: 0.9472\nEpoch 87/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0797 - auc: 0.9492 - val_loss: 0.0817 - val_auc: 0.9486\nEpoch 88/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0795 - auc: 0.9496 - val_loss: 0.0811 - val_auc: 0.9496\nEpoch 89/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0790 - auc: 0.9502 - val_loss: 0.0821 - val_auc: 0.9481\nEpoch 90/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0789 - auc: 0.9504 - val_loss: 0.0810 - val_auc: 0.9490\nEpoch 91/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0781 - auc: 0.9517 - val_loss: 0.0805 - val_auc: 0.9506\nEpoch 92/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0779 - auc: 0.9519 - val_loss: 0.0808 - val_auc: 0.9504\nEpoch 93/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0781 - auc: 0.9518 - val_loss: 0.0797 - val_auc: 0.9510\nEpoch 94/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0770 - auc: 0.9532 - val_loss: 0.0798 - val_auc: 0.9499\nEpoch 95/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0767 - auc: 0.9536 - val_loss: 0.0796 - val_auc: 0.9529\nEpoch 96/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0767 - auc: 0.9536 - val_loss: 0.0781 - val_auc: 0.9527\nEpoch 97/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0759 - auc: 0.9542 - val_loss: 0.0782 - val_auc: 0.9520\nEpoch 98/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0759 - auc: 0.9543 - val_loss: 0.0791 - val_auc: 0.9523\nEpoch 99/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0750 - auc: 0.9558 - val_loss: 0.0778 - val_auc: 0.9525\nEpoch 100/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0747 - auc: 0.9561 - val_loss: 0.0778 - val_auc: 0.9524\nEpoch 101/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0743 - auc: 0.9567 - val_loss: 0.0765 - val_auc: 0.9545\nEpoch 102/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0738 - auc: 0.9572 - val_loss: 0.0765 - val_auc: 0.9544\nEpoch 103/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0744 - auc: 0.9565 - val_loss: 0.0756 - val_auc: 0.9562\nEpoch 104/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0732 - auc: 0.9578 - val_loss: 0.0754 - val_auc: 0.9552\nEpoch 105/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0732 - auc: 0.9583 - val_loss: 0.0751 - val_auc: 0.9555\nEpoch 106/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0772 - auc: 0.9539 - val_loss: 0.0782 - val_auc: 0.9542\nEpoch 107/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0744 - auc: 0.9569 - val_loss: 0.0771 - val_auc: 0.9564\nEpoch 108/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0742 - auc: 0.9572 - val_loss: 0.0750 - val_auc: 0.9562\nEpoch 109/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0742 - auc: 0.9570 - val_loss: 0.0767 - val_auc: 0.9554\nEpoch 110/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0740 - auc: 0.9573 - val_loss: 0.0749 - val_auc: 0.9565\nEpoch 111/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0723 - auc: 0.9593 - val_loss: 0.0743 - val_auc: 0.9567\nEpoch 112/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0753 - auc: 0.9558 - val_loss: 0.0920 - val_auc: 0.9382\nEpoch 113/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0816 - auc: 0.9471 - val_loss: 0.0788 - val_auc: 0.9517\nEpoch 114/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0761 - auc: 0.9542 - val_loss: 0.0772 - val_auc: 0.9549\nEpoch 115/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0752 - auc: 0.9561 - val_loss: 0.0811 - val_auc: 0.9492\nEpoch 116/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0765 - auc: 0.9543 - val_loss: 0.0760 - val_auc: 0.9556\nEpoch 117/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0737 - auc: 0.9578 - val_loss: 0.0742 - val_auc: 0.9573\nEpoch 118/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0726 - auc: 0.9588 - val_loss: 0.0740 - val_auc: 0.9577\nEpoch 119/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0718 - auc: 0.9601 - val_loss: 0.0748 - val_auc: 0.9566\nEpoch 120/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0714 - auc: 0.9605 - val_loss: 0.0735 - val_auc: 0.9588\nEpoch 121/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0708 - auc: 0.9611 - val_loss: 0.0729 - val_auc: 0.9590\nEpoch 122/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0706 - auc: 0.9615 - val_loss: 0.0723 - val_auc: 0.9594\nEpoch 123/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0700 - auc: 0.9623 - val_loss: 0.0720 - val_auc: 0.9596\nEpoch 124/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0698 - auc: 0.9622 - val_loss: 0.0718 - val_auc: 0.9602\nEpoch 125/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0695 - auc: 0.9628 - val_loss: 0.0714 - val_auc: 0.9604\nEpoch 126/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0692 - auc: 0.9628 - val_loss: 0.0727 - val_auc: 0.9577\nEpoch 127/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0700 - auc: 0.9627 - val_loss: 0.0716 - val_auc: 0.9595\nEpoch 128/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0703 - auc: 0.9618 - val_loss: 0.0720 - val_auc: 0.9605\nEpoch 129/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0691 - auc: 0.9634 - val_loss: 0.0708 - val_auc: 0.9616\nEpoch 130/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0680 - auc: 0.9646 - val_loss: 0.0700 - val_auc: 0.9615\nEpoch 131/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0676 - auc: 0.9650 - val_loss: 0.0699 - val_auc: 0.9622\nEpoch 132/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0675 - auc: 0.9650 - val_loss: 0.0700 - val_auc: 0.9627\nEpoch 133/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0672 - auc: 0.9654 - val_loss: 0.0691 - val_auc: 0.9629\nEpoch 134/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0667 - auc: 0.9659 - val_loss: 0.0690 - val_auc: 0.9621\nEpoch 135/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0662 - auc: 0.9663 - val_loss: 0.0694 - val_auc: 0.9619\nEpoch 136/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0663 - auc: 0.9663 - val_loss: 0.0699 - val_auc: 0.9608\nEpoch 137/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0661 - auc: 0.9666 - val_loss: 0.0702 - val_auc: 0.9611\nEpoch 138/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0662 - auc: 0.9665 - val_loss: 0.0687 - val_auc: 0.9623\nEpoch 139/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0652 - auc: 0.9676 - val_loss: 0.0679 - val_auc: 0.9640\nEpoch 140/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0652 - auc: 0.9676 - val_loss: 0.0692 - val_auc: 0.9625\nEpoch 141/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0651 - auc: 0.9677 - val_loss: 0.0675 - val_auc: 0.9650\nEpoch 142/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0647 - auc: 0.9683 - val_loss: 0.0691 - val_auc: 0.9631\nEpoch 143/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0644 - auc: 0.9685 - val_loss: 0.0675 - val_auc: 0.9640\nEpoch 144/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0637 - auc: 0.9693 - val_loss: 0.0672 - val_auc: 0.9645\nEpoch 145/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0644 - auc: 0.9687 - val_loss: 0.0682 - val_auc: 0.9643\nEpoch 146/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0644 - auc: 0.9689 - val_loss: 0.0675 - val_auc: 0.9638\nEpoch 147/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0675 - auc: 0.9653 - val_loss: 0.0701 - val_auc: 0.9619\nEpoch 148/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0660 - auc: 0.9667 - val_loss: 0.0672 - val_auc: 0.9640\nEpoch 149/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0641 - auc: 0.9687 - val_loss: 0.0665 - val_auc: 0.9650\nEpoch 150/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0640 - auc: 0.9691 - val_loss: 0.0690 - val_auc: 0.9632\nEpoch 151/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0649 - auc: 0.9681 - val_loss: 0.0664 - val_auc: 0.9648\nEpoch 152/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0668 - auc: 0.9667 - val_loss: 0.0683 - val_auc: 0.9635\nEpoch 153/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0645 - auc: 0.9688 - val_loss: 0.0664 - val_auc: 0.9654\nEpoch 154/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0630 - auc: 0.9701 - val_loss: 0.0653 - val_auc: 0.9668\nEpoch 155/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0624 - auc: 0.9707 - val_loss: 0.0660 - val_auc: 0.9662\nEpoch 156/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0625 - auc: 0.9709 - val_loss: 0.0656 - val_auc: 0.9650\nEpoch 157/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0626 - auc: 0.9707 - val_loss: 0.0664 - val_auc: 0.9653\nEpoch 158/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0616 - auc: 0.9716 - val_loss: 0.0646 - val_auc: 0.9677\nEpoch 159/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0611 - auc: 0.9721 - val_loss: 0.0662 - val_auc: 0.9664\nEpoch 160/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0610 - auc: 0.9722 - val_loss: 0.0637 - val_auc: 0.9683\nEpoch 161/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0609 - auc: 0.9722 - val_loss: 0.0644 - val_auc: 0.9668\nEpoch 162/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0621 - auc: 0.9713 - val_loss: 0.0661 - val_auc: 0.9652\nEpoch 163/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0616 - auc: 0.9716 - val_loss: 0.0647 - val_auc: 0.9669\nEpoch 164/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0636 - auc: 0.9699 - val_loss: 0.0661 - val_auc: 0.9647\nEpoch 165/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0617 - auc: 0.9717 - val_loss: 0.0647 - val_auc: 0.9663\nEpoch 166/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0623 - auc: 0.9708 - val_loss: 0.0663 - val_auc: 0.9663\nEpoch 167/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0770 - auc: 0.9490 - val_loss: 0.0715 - val_auc: 0.9606\nEpoch 168/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0677 - auc: 0.9654 - val_loss: 0.0680 - val_auc: 0.9639\nEpoch 169/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0687 - auc: 0.9628 - val_loss: 0.0710 - val_auc: 0.9621\nEpoch 170/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0669 - auc: 0.9666 - val_loss: 0.0671 - val_auc: 0.9652\nEpoch 171/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0666 - auc: 0.9667 - val_loss: 0.0684 - val_auc: 0.9644\nEpoch 172/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0645 - auc: 0.9689 - val_loss: 0.0655 - val_auc: 0.9672\nEpoch 173/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0629 - auc: 0.9704 - val_loss: 0.0648 - val_auc: 0.9675\nEpoch 174/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0620 - auc: 0.9713 - val_loss: 0.0647 - val_auc: 0.9672\nEpoch 175/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0623 - auc: 0.9711 - val_loss: 0.0642 - val_auc: 0.9681\nEpoch 176/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0616 - auc: 0.9718 - val_loss: 0.0645 - val_auc: 0.9674\nEpoch 177/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0631 - auc: 0.9703 - val_loss: 0.0658 - val_auc: 0.9660\nEpoch 178/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0620 - auc: 0.9713 - val_loss: 0.0664 - val_auc: 0.9657\nEpoch 179/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0631 - auc: 0.9698 - val_loss: 0.0650 - val_auc: 0.9655\nEpoch 180/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0628 - auc: 0.9701 - val_loss: 0.0656 - val_auc: 0.9667\nEpoch 181/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0611 - auc: 0.9724 - val_loss: 0.0635 - val_auc: 0.9685\nEpoch 182/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0602 - auc: 0.9731 - val_loss: 0.0636 - val_auc: 0.9676\nEpoch 183/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0594 - auc: 0.9739 - val_loss: 0.0631 - val_auc: 0.9680\nEpoch 184/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0592 - auc: 0.9741 - val_loss: 0.0627 - val_auc: 0.9688\nEpoch 185/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0589 - auc: 0.9744 - val_loss: 0.0639 - val_auc: 0.9673\nEpoch 186/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0596 - auc: 0.9737 - val_loss: 0.0622 - val_auc: 0.9701\nEpoch 187/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0612 - auc: 0.9701 - val_loss: 0.0695 - val_auc: 0.9622\nEpoch 188/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0624 - auc: 0.9711 - val_loss: 0.0639 - val_auc: 0.9681\nEpoch 189/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0598 - auc: 0.9736 - val_loss: 0.0632 - val_auc: 0.9681\nEpoch 190/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0612 - auc: 0.9725 - val_loss: 0.0663 - val_auc: 0.9660\nEpoch 191/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0604 - auc: 0.9734 - val_loss: 0.0631 - val_auc: 0.9680\nEpoch 192/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0587 - auc: 0.9746 - val_loss: 0.0618 - val_auc: 0.9699\nEpoch 193/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0579 - auc: 0.9753 - val_loss: 0.0614 - val_auc: 0.9697\nEpoch 194/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0577 - auc: 0.9756 - val_loss: 0.0618 - val_auc: 0.9692\nEpoch 195/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0573 - auc: 0.9759 - val_loss: 0.0610 - val_auc: 0.9703\nEpoch 196/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0569 - auc: 0.9763 - val_loss: 0.0613 - val_auc: 0.9704\nEpoch 197/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0565 - auc: 0.9767 - val_loss: 0.0611 - val_auc: 0.9701\nEpoch 198/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0569 - auc: 0.9763 - val_loss: 0.0645 - val_auc: 0.9669\nEpoch 199/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0580 - auc: 0.9755 - val_loss: 0.0614 - val_auc: 0.9720\nEpoch 200/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0569 - auc: 0.9766 - val_loss: 0.0612 - val_auc: 0.9716\nEpoch 201/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0563 - auc: 0.9770 - val_loss: 0.0602 - val_auc: 0.9722\nEpoch 202/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0576 - auc: 0.9761 - val_loss: 0.0635 - val_auc: 0.9691\nEpoch 203/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0569 - auc: 0.9767 - val_loss: 0.0600 - val_auc: 0.9725\nEpoch 204/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0557 - auc: 0.9777 - val_loss: 0.0610 - val_auc: 0.9712\nEpoch 205/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0554 - auc: 0.9778 - val_loss: 0.0612 - val_auc: 0.9702\nEpoch 206/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0554 - auc: 0.9779 - val_loss: 0.0596 - val_auc: 0.9722\nEpoch 207/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0547 - auc: 0.9784 - val_loss: 0.0605 - val_auc: 0.9716\nEpoch 208/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0548 - auc: 0.9784 - val_loss: 0.0595 - val_auc: 0.9720\nEpoch 209/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0545 - auc: 0.9786 - val_loss: 0.0594 - val_auc: 0.9722\nEpoch 210/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0557 - auc: 0.9779 - val_loss: 0.0613 - val_auc: 0.9702\nEpoch 211/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0551 - auc: 0.9782 - val_loss: 0.0593 - val_auc: 0.9727\nEpoch 212/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0545 - auc: 0.9787 - val_loss: 0.0593 - val_auc: 0.9732\nEpoch 213/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0541 - auc: 0.9791 - val_loss: 0.0588 - val_auc: 0.9729\nEpoch 214/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0533 - auc: 0.9797 - val_loss: 0.0584 - val_auc: 0.9738\nEpoch 215/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0532 - auc: 0.9796 - val_loss: 0.0597 - val_auc: 0.9733\nEpoch 216/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0543 - auc: 0.9790 - val_loss: 0.0597 - val_auc: 0.9720\nEpoch 217/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0533 - auc: 0.9797 - val_loss: 0.0590 - val_auc: 0.9724\nEpoch 218/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0535 - auc: 0.9794 - val_loss: 0.0587 - val_auc: 0.9725\nEpoch 219/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0531 - auc: 0.9798 - val_loss: 0.0580 - val_auc: 0.9741\nEpoch 220/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0528 - auc: 0.9801 - val_loss: 0.0584 - val_auc: 0.9744\nEpoch 221/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0574 - auc: 0.9763 - val_loss: 0.0605 - val_auc: 0.9731\nEpoch 222/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0545 - auc: 0.9789 - val_loss: 0.0589 - val_auc: 0.9732\nEpoch 223/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0542 - auc: 0.9790 - val_loss: 0.0594 - val_auc: 0.9713\nEpoch 224/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0536 - auc: 0.9796 - val_loss: 0.0581 - val_auc: 0.9739\nEpoch 225/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0527 - auc: 0.9804 - val_loss: 0.0584 - val_auc: 0.9734\nEpoch 226/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0523 - auc: 0.9806 - val_loss: 0.0581 - val_auc: 0.9732\nEpoch 227/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0521 - auc: 0.9806 - val_loss: 0.0588 - val_auc: 0.9726\nEpoch 228/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0534 - auc: 0.9797 - val_loss: 0.0607 - val_auc: 0.9722\nEpoch 229/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0524 - auc: 0.9805 - val_loss: 0.0586 - val_auc: 0.9721\nEpoch 230/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0513 - auc: 0.9812 - val_loss: 0.0573 - val_auc: 0.9741\nEpoch 231/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0510 - auc: 0.9815 - val_loss: 0.0580 - val_auc: 0.9731\nEpoch 232/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0514 - auc: 0.9811 - val_loss: 0.0578 - val_auc: 0.9741\nEpoch 233/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0513 - auc: 0.9814 - val_loss: 0.0573 - val_auc: 0.9734\nEpoch 234/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0507 - auc: 0.9820 - val_loss: 0.0596 - val_auc: 0.9711\nEpoch 235/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0510 - auc: 0.9815 - val_loss: 0.0571 - val_auc: 0.9740\nEpoch 236/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0500 - auc: 0.9823 - val_loss: 0.0570 - val_auc: 0.9745\nEpoch 237/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0504 - auc: 0.9821 - val_loss: 0.0576 - val_auc: 0.9740\nEpoch 238/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0503 - auc: 0.9822 - val_loss: 0.0589 - val_auc: 0.9739\nEpoch 239/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0505 - auc: 0.9822 - val_loss: 0.0569 - val_auc: 0.9746\nEpoch 240/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0508 - auc: 0.9820 - val_loss: 0.0581 - val_auc: 0.9738\nEpoch 241/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0494 - auc: 0.9830 - val_loss: 0.0575 - val_auc: 0.9737\nEpoch 242/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0494 - auc: 0.9828 - val_loss: 0.0573 - val_auc: 0.9737\nEpoch 243/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0492 - auc: 0.9829 - val_loss: 0.0575 - val_auc: 0.9740\nEpoch 244/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0495 - auc: 0.9828 - val_loss: 0.0578 - val_auc: 0.9741\nEpoch 245/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0519 - auc: 0.9812 - val_loss: 0.0587 - val_auc: 0.9731\nEpoch 246/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0505 - auc: 0.9822 - val_loss: 0.0564 - val_auc: 0.9755\nEpoch 247/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0489 - auc: 0.9833 - val_loss: 0.0580 - val_auc: 0.9731\nEpoch 248/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0487 - auc: 0.9833 - val_loss: 0.0566 - val_auc: 0.9743\nEpoch 249/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0480 - auc: 0.9840 - val_loss: 0.0565 - val_auc: 0.9746\nEpoch 250/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0478 - auc: 0.9840 - val_loss: 0.0563 - val_auc: 0.9747\nEpoch 251/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0525 - auc: 0.9806 - val_loss: 0.0601 - val_auc: 0.9722\nEpoch 252/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0506 - auc: 0.9822 - val_loss: 0.0570 - val_auc: 0.9748\nEpoch 253/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0485 - auc: 0.9836 - val_loss: 0.0570 - val_auc: 0.9748\nEpoch 254/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0477 - auc: 0.9842 - val_loss: 0.0564 - val_auc: 0.9743\nEpoch 255/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0493 - auc: 0.9832 - val_loss: 0.0574 - val_auc: 0.9735\nEpoch 256/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0476 - auc: 0.9841 - val_loss: 0.0566 - val_auc: 0.9741\nEpoch 257/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0476 - auc: 0.9841 - val_loss: 0.0569 - val_auc: 0.9740\nEpoch 258/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0472 - auc: 0.9844 - val_loss: 0.0566 - val_auc: 0.9739\nEpoch 259/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0475 - auc: 0.9840 - val_loss: 0.0577 - val_auc: 0.9740\nEpoch 260/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0473 - auc: 0.9846 - val_loss: 0.0594 - val_auc: 0.9744\nEpoch 261/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0525 - auc: 0.9811 - val_loss: 0.0562 - val_auc: 0.9749\nEpoch 262/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0476 - auc: 0.9843 - val_loss: 0.0560 - val_auc: 0.9753\nEpoch 263/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0466 - auc: 0.9851 - val_loss: 0.0583 - val_auc: 0.9738\nEpoch 264/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0526 - auc: 0.9809 - val_loss: 0.0576 - val_auc: 0.9746\nEpoch 265/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0498 - auc: 0.9830 - val_loss: 0.0578 - val_auc: 0.9748\nEpoch 266/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0481 - auc: 0.9842 - val_loss: 0.0563 - val_auc: 0.9751\nEpoch 267/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0470 - auc: 0.9850 - val_loss: 0.0565 - val_auc: 0.9750\nEpoch 268/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0468 - auc: 0.9850 - val_loss: 0.0562 - val_auc: 0.9749\nEpoch 269/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0459 - auc: 0.9855 - val_loss: 0.0556 - val_auc: 0.9748\nEpoch 270/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0456 - auc: 0.9857 - val_loss: 0.0561 - val_auc: 0.9754\nEpoch 271/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0455 - auc: 0.9858 - val_loss: 0.0571 - val_auc: 0.9750\nEpoch 272/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0464 - auc: 0.9852 - val_loss: 0.0562 - val_auc: 0.9747\nEpoch 273/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0455 - auc: 0.9858 - val_loss: 0.0567 - val_auc: 0.9747\nEpoch 274/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0455 - auc: 0.9858 - val_loss: 0.0567 - val_auc: 0.9748\nEpoch 275/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0451 - auc: 0.9861 - val_loss: 0.0556 - val_auc: 0.9748\nEpoch 276/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0454 - auc: 0.9859 - val_loss: 0.0557 - val_auc: 0.9753\nEpoch 277/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0444 - auc: 0.9864 - val_loss: 0.0560 - val_auc: 0.9749\nEpoch 278/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0478 - auc: 0.9840 - val_loss: 0.0598 - val_auc: 0.9719\nEpoch 279/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0482 - auc: 0.9840 - val_loss: 0.0565 - val_auc: 0.9751\nEpoch 280/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0456 - auc: 0.9858 - val_loss: 0.0562 - val_auc: 0.9757\nEpoch 281/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0451 - auc: 0.9860 - val_loss: 0.0558 - val_auc: 0.9751\nEpoch 282/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0450 - auc: 0.9860 - val_loss: 0.0557 - val_auc: 0.9756\nEpoch 283/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0445 - auc: 0.9862 - val_loss: 0.0560 - val_auc: 0.9753\nEpoch 284/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0440 - auc: 0.9866 - val_loss: 0.0557 - val_auc: 0.9752\nEpoch 285/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0436 - auc: 0.9868 - val_loss: 0.0560 - val_auc: 0.9761\nEpoch 286/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0434 - auc: 0.9870 - val_loss: 0.0553 - val_auc: 0.9753\nEpoch 287/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0431 - auc: 0.9872 - val_loss: 0.0558 - val_auc: 0.9754\nEpoch 288/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0442 - auc: 0.9865 - val_loss: 0.0554 - val_auc: 0.9751\nEpoch 289/400\n70/70 [==============================] - 98s 1s/step - loss: 0.0428 - auc: 0.9872 - val_loss: 0.0571 - val_auc: 0.9750\nEpoch 290/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0438 - auc: 0.9870 - val_loss: 0.0564 - val_auc: 0.9751\nEpoch 291/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0433 - auc: 0.9871 - val_loss: 0.0558 - val_auc: 0.9752\nEpoch 292/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0428 - auc: 0.9875 - val_loss: 0.0559 - val_auc: 0.9749\nEpoch 293/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0442 - auc: 0.9865 - val_loss: 0.0557 - val_auc: 0.9752\nEpoch 294/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0432 - auc: 0.9870 - val_loss: 0.0571 - val_auc: 0.9747\nEpoch 295/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0436 - auc: 0.9870 - val_loss: 0.0558 - val_auc: 0.9745\nEpoch 296/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0424 - auc: 0.9876 - val_loss: 0.0560 - val_auc: 0.9747\nEpoch 297/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0426 - auc: 0.9875 - val_loss: 0.0563 - val_auc: 0.9748\nEpoch 298/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0424 - auc: 0.9877 - val_loss: 0.0568 - val_auc: 0.9739\nEpoch 299/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0418 - auc: 0.9879 - val_loss: 0.0562 - val_auc: 0.9742\nEpoch 300/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0417 - auc: 0.9881 - val_loss: 0.0580 - val_auc: 0.9730\nEpoch 301/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0421 - auc: 0.9878 - val_loss: 0.0556 - val_auc: 0.9752\nEpoch 302/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0424 - auc: 0.9878 - val_loss: 0.0561 - val_auc: 0.9747\nEpoch 303/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0446 - auc: 0.9864 - val_loss: 0.0558 - val_auc: 0.9738\nEpoch 304/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0421 - auc: 0.9878 - val_loss: 0.0565 - val_auc: 0.9738\nEpoch 305/400\n70/70 [==============================] - 97s 1s/step - loss: 0.0415 - auc: 0.9882 - val_loss: 0.0566 - val_auc: 0.9739\nEpoch 306/400\n70/70 [==============================] - 96s 1s/step - loss: 0.0416 - auc: 0.9881 - val_loss: 0.0561 - val_auc: 0.9747\nEpoch 307/400\n17/70 [======>.......................] - ETA: 1:10 - loss: 0.0439 - auc: 0.9866","output_type":"stream"}]},{"cell_type":"code","source":"!cd /kaggle/working\n#!ls\n!zip -r all_bidir.zip /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-07-03T00:47:10.300653Z","iopub.status.idle":"2023-07-03T00:47:10.301714Z","shell.execute_reply.started":"2023-07-03T00:47:10.301457Z","shell.execute_reply":"2023-07-03T00:47:10.301481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Загрузка данных\nX_train, y_train, X_test, y_test = load_ptbxl(task = 'all')\n\n# Обучение модели\nmodel = M_ATI_CNN(num_classes=71, multi_label_classifier=True)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=[keras.losses.BinaryCrossentropy()],\n    metrics=[keras.metrics.AUC()]\n)\ncallbacks = [\n    keras.callbacks.EarlyStopping(\n        # Прекратить обучение если `val_loss` больше не улучшается\n        monitor='val_auc',\n        mode='max',\n        # \"больше не улучшается\" определим как \"не лучше чем min_delta и меньше\"\n        min_delta=1e-2,\n        # \"больше не улучшается\" далее определим как \"как минимум в течение patience эпох\"\n        patience=5,\n        verbose=1\n    ),\n    keras.callbacks.ModelCheckpoint(\n        filepath='M_ATI_CNN_ML.h5',\n        # Путь по которому нужно сохранить модель\n        # Два параметра ниже значат что мы перезапишем\n        # текущий чекпоинт в том и только в том случае, когда\n        # улучится значение `val_loss`.\n        save_best_only=True,\n        monitor='val_auc',\n        mode='max',\n        verbose=1\n    )\n]\nhistory = model.fit(X_train, y_train,\n        validation_data=0.1,\n        epochs=100,\n        batch_size=64,\n        callbacks=callbacks,\n        shuffle=True)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd /kaggle/working\n#!ls\n!zip -r superdiag.zip /kaggle/working","metadata":{},"execution_count":null,"outputs":[]}]}