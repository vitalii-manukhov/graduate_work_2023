{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пакет os нужен для перемещения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/input/icbebnpy\") # Перейдем в Input (только для чтения!)\n",
    "!ls # Посмотреть содержимое\n",
    "os.chdir(\"/kaggle/working/\") # Перейдем в Output\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-30T00:41:55.070918Z",
     "iopub.status.busy": "2023-01-30T00:41:55.070515Z",
     "iopub.status.idle": "2023-01-30T00:41:55.170474Z",
     "shell.execute_reply": "2023-01-30T00:41:55.169276Z",
     "shell.execute_reply.started": "2023-01-30T00:41:55.070887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Для работы с данными\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "import seaborn as sns   # plotting heatmap\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "# Для работы с моделями\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import load_model\n",
    "# Для метрик\n",
    "from keras import backend as K\n",
    "from keras.metrics import AUC, Recall, Precision, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n",
    "from sklearn.metrics import fbeta_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "# Код из учебника Жерона \n",
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.prev_loss = 0\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        batch_loss = logs[\"loss\"] * (batch + 1) - self.prev_loss * batch\n",
    "        self.prev_loss = logs[\"loss\"]\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(batch_loss)\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)\n",
    "        \n",
    "# Загрузка ICBEB\n",
    "def load_ICBEB(task):\n",
    "  #os.chdir(\"/kaggle/input/icbeb2023\")\n",
    "  if task == 'all':\n",
    "    X_train = np.load('data_npy/ICBEBnpy/X_train_ICBEB_all.npy')\n",
    "    y_train = np.load('data_npy/ICBEBnpy/y_train_ICBEB_all.npy')\n",
    "    X_test = np.load('data_npy/ICBEBnpy/X_val_ICBEB_all.npy')\n",
    "    y_test = np.load('data_npy/ICBEBnpy/y_val_ICBEB_all.npy')\n",
    "  #os.chdir(\"/kaggle/working/\") \n",
    "  return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Сериализация\n",
    "def sp(obj, name): # sp = serialization_pickle ;name = *.pkl\n",
    "    with open(name, 'wb') as pickle_out:\n",
    "        pickle.dump(obj, pickle_out)\n",
    "\n",
    "# Компиляция и обучение модели\n",
    "def AUC_Keras(y_true, y_pred):\n",
    "    auc = keras.metrics.AUC(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "# Компиляция и обучение модели\n",
    "def compile_fit(model, X_train, y_train, X_val = None, y_val = None, validation_split = 0.0, batch_size = None, epochs = None, early_stopping = None, model_checkpoint = None, onecycle = None):\n",
    "  model.compile(loss = keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.experimental.AdamW(),\n",
    "                metrics=['AUC'])\n",
    "  \n",
    "  if X_val == None:\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs = epochs, \n",
    "                        batch_size = batch_size,\n",
    "                        validation_data = None, \n",
    "                        validation_split=validation_split, \n",
    "                        callbacks=[onecycle, model_checkpoint, early_stopping])\n",
    "    sp(history, 'history.pkl')\n",
    "  else:\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs = epochs,\n",
    "                        batch_size = batch_size, \n",
    "                        validation_data = (X_val, y_val), \n",
    "                        validation_split=0.0, \n",
    "                        callbacks=[onecycle, model_checkpoint, early_stopping])\n",
    "    sp(history, 'history.pkl')\n",
    "  return history\n",
    "\n",
    "# TP TN FP FN\n",
    "def tp_tn_fp_fn(y_true, y_pred):\n",
    "  TP = TruePositives()\n",
    "  TN = TrueNegatives()\n",
    "  FP = FalsePositives()\n",
    "  FN = FalseNegatives()\n",
    "  TP.update_state(y_true, y_pred)\n",
    "  TN.update_state(y_true, y_pred)\n",
    "  FP.update_state(y_true, y_pred)\n",
    "  FN.update_state(y_true, y_pred)\n",
    "  return TP.result().numpy(),  TN.result().numpy(),  FP.result().numpy(), FN.result().numpy() \n",
    "\n",
    "# Подсчет метрик\n",
    "def calc_metrics(t, p, flag = 0): # t - y_true, p - y_pred\n",
    "  y_true=np.argmax(t, axis=1)\n",
    "  y_pred=np.argmax(p, axis=1)\n",
    "  beta = 2\n",
    "\n",
    "  f2_score = fbeta_score(y_true, y_pred, average='macro', beta=2)\n",
    "  precision = precision_score(y_true, y_pred, average='macro')\n",
    "  recall = recall_score(y_true, y_pred, average='macro')\n",
    "  TP, TN, FP, FN = tp_tn_fp_fn(t, p)\n",
    "  g2_score = TP/(TP+FP+beta*FN)\n",
    "    \n",
    "  sp(f2_score, 'f2_score.pkl')\n",
    "  sp(g2_score, 'g2_score.pkl')\n",
    "  sp(precision, 'precision.pkl')\n",
    "  sp(recall, 'recall.pkl')\n",
    "  sp(TP, 'TP.pkl')\n",
    "  sp(TN, 'TN.pkl')\n",
    "  sp(FP, 'FP.pkl')\n",
    "  sp(FN, 'FN.pkl')\n",
    "  sp(y_true, 'y_true_argmax.pkl')\n",
    "  sp(y_pred, 'y_pred_argmax.pkl')\n",
    "    \n",
    "  if flag == 0:\n",
    "    return f2_score, g2_score\n",
    "  elif flag == 1:\n",
    "    return f2_score, g2_score, precision, recall\n",
    "\n",
    "# Таблица результатов\n",
    "table_res_finetuning = pd.DataFrame(columns = ('AUC', 'F2', 'G2'))\n",
    "\n",
    "# Занесение новых результатов в таблицу\n",
    "def edit_table(table, model, X, y, index_name): # X - X_test, y - y_test \n",
    "  score = model.evaluate(X, y)\n",
    "  y_pr = model.predict(X)\n",
    "  f2_score, g2_score = calc_metrics(y, y_pr, flag = 0)\n",
    "  list_metrics = [score[1], f2_score, g2_score] # AUC, F2, G2\n",
    "  table.loc[index_name] = list_metrics\n",
    "\n",
    "  sp(score, 'score.pkl')\n",
    "  sp(y_pr, 'y_pred.pkl')\n",
    "  sp(y, 'y_test.pkl')\n",
    "    \n",
    "  return table\n",
    "\n",
    "# График loss и accuracy\n",
    "def plot_loss_and_accuracy_curves(_history):\n",
    "  fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18,6))\n",
    "  axs[0].plot(_history.history['loss'], color='b', label='Training loss')\n",
    "  axs[0].plot(_history.history['val_loss'], color='r', label='Validation loss')\n",
    "  axs[0].set_title(\"Loss\")\n",
    "  axs[0].legend(loc='best', shadow=True)\n",
    "  axs[1].plot(_history.history['auc'], color='b', label='Training accuracy')\n",
    "  axs[1].plot(_history.history['val_auc'], color='r', label='Validation accuracy')\n",
    "  axs[1].set_title(\"AUC\")\n",
    "  axs[1].legend(loc='best', shadow=True)\n",
    "  plt.show()\n",
    "  \n",
    "# Работа с моделями lstm и lstm_bidir\n",
    "def type_comp_fit_save_model_score(table, X_train, y_train, X_test, y_test, type_model, save_name, index_model_task, batch_size, epochs, onecycle):\n",
    "  # количество классов\n",
    "  num_classes = y_train.shape[1]\n",
    "  sp(num_classes, 'num_classes.pkl')\n",
    "\n",
    "  # Выбор архитектуры модели\n",
    "  if type_model == 'lstm':\n",
    "    inputs = keras.Input(shape=(1000, 12))\n",
    "    x = layers.LSTM(units=256,\n",
    "                    return_sequences=True,\n",
    "                    stateful=False,\n",
    "                    unroll=False)(inputs)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.LSTM(units=256,\n",
    "                    return_sequences=True, # True if concat\n",
    "                    stateful=False,\n",
    "                    unroll=False)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "    max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "    concat = layers.Concatenate()([avg_pool, max_pool])\n",
    "    outputs = layers.Dense(units=num_classes, activation='softmax')(concat)\n",
    "    #outputs = layers.Dense(units=num_classes, activation='softmax')(x) # if concat off  \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Реализация раннего прекращения.\n",
    "    checkpoint_filepath = './checkpoint_lstm/'\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                          save_weights_only=True,\n",
    "                                                          save_best_only=True)\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=epochs//10,\n",
    "                                                  restore_best_weights=True)\n",
    "  elif type_model == 'lstm_bidir':\n",
    "    inputs = keras.Input(shape=(1000, 12))\n",
    "    x = layers.Bidirectional(layers.LSTM(units=256,\n",
    "                             return_sequences = True,\n",
    "                             stateful = False,\n",
    "                             unroll = False))(inputs)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(units=256,\n",
    "                             return_sequences = True,\n",
    "                             stateful = False,\n",
    "                             unroll = False))(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "    max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "    concat = layers.Concatenate()([avg_pool, max_pool])\n",
    "    outputs = layers.Dense(units=num_classes, activation='softmax')(concat)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.build(input_shape = (None, 1000, 12)) # `input_shape` is the shape of the input data\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    # Реализация раннего прекращения.\n",
    "    checkpoint_filepath = './checkpoint_lstm_bidir/'\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                          save_weights_only=True,\n",
    "                                                          save_best_only=True)\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=epochs//10,\n",
    "                                                  restore_best_weights=True)\n",
    "  \n",
    "  # Обучение\n",
    "  History = compile_fit(model, X_train, y_train, validation_split=0.1, batch_size=batch_size, epochs=epochs, early_stopping=early_stopping, model_checkpoint=model_checkpoint, onecycle=onecycle)\n",
    "\n",
    "  # Сохранение модели\n",
    "  model.save(save_name)\n",
    "\n",
    "  # Построение графика\n",
    "  plot_loss_and_accuracy_curves(History)\n",
    "\n",
    "  # Сохранение в таблицу\n",
    "  table = edit_table(table, model, X_test, y_test, index_model_task)\n",
    "\n",
    "  sp(table, 'table.pkl')\n",
    "  return table\n",
    "                        \n",
    "tf.random.set_seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6187 (6187, 1000, 12)\n",
      "96 (96, 1000, 12)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Размеры на выбор в p раз меньше: P = {1, 2, 4, 8, 16, 32, 64}\n",
    "def data_new_size(X, y, p = 1): # p - во сколько раз уменьшить (минимум 1 (default), максимум 64) \n",
    "\n",
    "    # Размер нового набора данных (в p раз меньше исходного)\n",
    "    new_size = len(X) // p\n",
    "\n",
    "    # Случайная выборка нового размера\n",
    "    random_indices = random.sample(range(len(X_train)), new_size)\n",
    "    X_batch = np.take(X, random_indices, axis=0)\n",
    "    y_batch = np.take(y, random_indices, axis=0)\n",
    "    return X_batch, y_batch\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_ICBEB(task = 'all')\n",
    "print(len(X_train), X_train.shape)\n",
    "\n",
    "X_train, y_train = data_new_size(X_train, y_train, 64)\n",
    "print(len(X_train), X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T01:31:25.210924Z",
     "iopub.status.busy": "2023-01-30T01:31:25.210547Z",
     "iopub.status.idle": "2023-01-30T01:32:08.747299Z",
     "shell.execute_reply": "2023-01-30T01:32:08.746371Z",
     "shell.execute_reply.started": "2023-01-30T01:31:25.210894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1000, 12)]   0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 1000, 256)    275456      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 1000, 256)    0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 1000, 256)    525312      ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 1000, 256)    0           ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 256)         0           ['leaky_re_lu_1[0][0]']          \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 256)         0           ['leaky_re_lu_1[0][0]']          \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 512)          0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 , 'global_max_pooling1d[0][0]']  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 9)            4617        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 805,385\n",
      "Trainable params: 4,617\n",
      "Non-trainable params: 800,768\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/3 [=========>....................] - ETA: 28s - loss: 0.9271 - auc: 0.4963"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 41\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Компиляция и обучение модели\u001b[39;00m\n\u001b[0;32m     38\u001b[0m lstm\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBinaryCrossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     39\u001b[0m              optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAdamW(),\n\u001b[0;32m     40\u001b[0m              metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 41\u001b[0m History \u001b[38;5;241m=\u001b[39m \u001b[43mlstm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monecycle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Сохранение модели\u001b[39;00m\n\u001b[0;32m     48\u001b[0m lstm\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_finetuning_form\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mE:\\Временное хранилище\\Repository\\practice_2022-2023\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mE:\\Временное хранилище\\Repository\\practice_2022-2023\\env\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mE:\\Временное хранилище\\Repository\\practice_2022-2023\\env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mE:\\Временное хранилище\\Repository\\practice_2022-2023\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mE:\\Временное хранилище\\Repository\\practice_2022-2023\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mE:\\Временное хранилище\\Repository\\practice_2022-2023\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Временное хранилище\\Repository\\practice_2022-2023\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mE:\\Временное хранилище\\Repository\\practice_2022-2023\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mE:\\Временное хранилище\\Repository\\practice_2022-2023\\env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_classes = y_train.shape[1]\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "base_lstm = load_model('model_lstm_all') # Загрузка предобученной модели\n",
    "base_lstm.trainable = False  # Замораживаем все слои предобученной модели\n",
    "# # ИЛИ\n",
    "# for layer in base_lstm.layers: # Заморозка всех слоев обученной модели\n",
    "#     layer.trainable = False\n",
    "\n",
    "inputs = base_lstm.layers[0].input  # Используем входной тензор первого слоя в base_lstm\n",
    "x = base_lstm.layers[-2].output  # Используем выходной тензор предпоследнего слоя в base_lstm\n",
    "x = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "lstm = keras.Model(inputs=inputs, outputs=x)\n",
    "lstm.summary()\n",
    "\n",
    "num_unfreeze_layers = 8\n",
    "learning_rates = 0.001\n",
    "# Цикл по слоям для постепенного размораживания и установки скоростей обучения\n",
    "for i, layer in enumerate(lstm.layers):\n",
    "    if i < num_unfreeze_layers:\n",
    "        layer.trainable = True  # Размораживание слоя\n",
    "        layer.learning_rate = learning_rates # Установка дифференцированной скорости обучения для слоя\n",
    "        learning_rates = learning_rates * 0.1\n",
    "\n",
    "# 1-cycle\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * epochs, max_rate=0.0001)\n",
    "\n",
    "# Реализация раннего прекращения.\n",
    "checkpoint_filepath = './checkpoint_lstm/'\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                      save_weights_only=True,\n",
    "                                                      save_best_only=True)\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=15,\n",
    "                                              restore_best_weights=True)\n",
    "\n",
    "# Компиляция и обучение модели\n",
    "lstm.compile(loss='BinaryCrossentropy',\n",
    "             optimizer=keras.optimizers.experimental.AdamW(),\n",
    "             metrics=['AUC'])\n",
    "History = lstm.fit(X_train, y_train,\n",
    "                   validation_split=0.1,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=epochs,\n",
    "                   callbacks=[model_checkpoint, early_stopping, onecycle])\n",
    "\n",
    "sp(History, 'History.pkl')\n",
    "\n",
    "# Сохранение модели\n",
    "lstm.save_model('lstm_finetuning_form')\n",
    "\n",
    "# Построение графика\n",
    "plot_loss_and_accuracy_curves(History)\n",
    "\n",
    "# Запись результатов\n",
    "table_res_finetuning = edit_table(table_res_finetuning, lstm, X_test, y_test, 'lstm_finetuning_form')\n",
    "sp(Table, 'Table.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /kaggle/working\n",
    "#!ls\n",
    "!zip -r all_bidir.zip /kaggle/working"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
