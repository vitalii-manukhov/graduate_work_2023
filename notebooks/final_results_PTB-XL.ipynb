{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c2e226",
   "metadata": {},
   "source": [
    "Цель: привести две набора данных: провести эксперименты с моделями (lstm, lstm_bidir, ATI-CNN_ML, M_MBLF_ML, DNN_ML) c предобработкой и без предобработки. Сначала проведем эксперименты с моделями на непредобработанных данных, а потом на предобработанных данных. Предобработка данных будет заключаться в наработках Неймарка и мб ещё что-нибудь. В роли данных будут выступать 'all', возможно ещё 'diag', а также менее вероятно 'superdiag' и 'subdiag'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e68b5",
   "metadata": {},
   "source": [
    "Сначала попробуем на google collab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2755b7a0",
   "metadata": {},
   "source": [
    "### Для Google Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df46178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подключение Google Drive к виртуальной машине\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Копирование данных с Google Drive на локальный диск виртуальной машины.\n",
    "!cp -r /content/drive/MyDrive/practice_2022-2023/data/ptbxlnpy/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f332c",
   "metadata": {},
   "source": [
    "### Импорт библиотек, определение функций и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc74a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для работы с данными\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "import seaborn as sns   # plotting heatmap\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "# Для работы с моделями\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Для метрик\n",
    "from keras import backend as K\n",
    "from keras.metrics import AUC, Recall, Precision, Accuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n",
    "from sklearn.metrics import fbeta_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import auc, roc_curve, classification_report\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "# Код из учебника Жерона \n",
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.prev_loss = 0\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        batch_loss = logs[\"loss\"] * (batch + 1) - self.prev_loss * batch\n",
    "        self.prev_loss = logs[\"loss\"]\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(batch_loss)\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)\n",
    "        \n",
    "class attention(layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1),\n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1),\n",
    "                               initializer='zeros', trainable=True)\n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)\n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "def ATI_CNN(num_classes, multi_label_classifier=False):\n",
    "    activation = keras.activations.sigmoid if multi_label_classifier else keras.activations.softmax\n",
    "\n",
    "    def Conv_block_1(filters):\n",
    "        return keras.Sequential([\n",
    "            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.MaxPooling1D(pool_size=3, strides=3),\n",
    "            layers.Dropout(rate=0.2)\n",
    "        ])\n",
    "\n",
    "    def Conv_block_2(filters):\n",
    "        return keras.Sequential([\n",
    "            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.MaxPooling1D(pool_size=3, strides=3),\n",
    "            layers.Dropout(rate=0.2)\n",
    "        ])\n",
    "\n",
    "    shape = (1000, 12)\n",
    "    input = keras.Input(shape=shape, name=\"ECG\")\n",
    "\n",
    "    x = Conv_block_1(64)(input)\n",
    "    x = Conv_block_1(128)(x)\n",
    "\n",
    "    x = Conv_block_2(256)(x)\n",
    "    x = Conv_block_2(256)(x)\n",
    "    x = Conv_block_2(256)(x)\n",
    "\n",
    "    x = layers.LSTM(units=32, return_sequences=True)(x)\n",
    "    x = layers.LSTM(units=32, return_sequences=True)(x)\n",
    "\n",
    "    x = attention()(x)\n",
    "\n",
    "    output = layers.Dense(units=num_classes, activation=activation, name='output')(x)\n",
    "\n",
    "    model = keras.Model(inputs=[input], outputs=[output])\n",
    "\n",
    "    return model\n",
    "\n",
    "def M_ATI_CNN(num_classes, multi_label_classifier=False):\n",
    "    activation = layers.Activation(keras.activations.sigmoid) if multi_label_classifier else layers.Activation(keras.activations.softmax)\n",
    "\n",
    "    def Conv_block_1(filters):\n",
    "        return keras.Sequential([\n",
    "            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.MaxPooling1D(pool_size=3, strides=3),\n",
    "            layers.Dropout(rate=0.2)\n",
    "        ])\n",
    "\n",
    "    def Conv_block_2(filters):\n",
    "        return keras.Sequential([\n",
    "            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv1D(filters=filters, kernel_size=3, strides=1, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.MaxPooling1D(pool_size=3, strides=3),\n",
    "            layers.Dropout(rate=0.2)\n",
    "        ])\n",
    "\n",
    "    shape = (1000, 12)\n",
    "    input = keras.Input(shape=shape, name=\"ECG\")\n",
    "\n",
    "    output_modules = list()\n",
    "\n",
    "    for i in range(num_classes):\n",
    "      output_modules.append(keras.Sequential([\n",
    "          Conv_block_1(64),\n",
    "          Conv_block_1(128),\n",
    "          Conv_block_2(256),\n",
    "          Conv_block_2(256),\n",
    "          Conv_block_2(256)\n",
    "      ])(input))\n",
    "\n",
    "      output_modules[-1] = layers.Bidirectional(layers.LSTM(units=32, return_sequences=True))(output_modules[-1])\n",
    "\n",
    "      output_modules[-1] = attention()(output_modules[-1])\n",
    "      output_modules[-1] = layers.Dense(units=1)(output_modules[-1])\n",
    "\n",
    "    cat_outputs =  keras.layers.Concatenate()(output_modules)\n",
    "\n",
    "    output = activation(cat_outputs)\n",
    "\n",
    "    model = keras.Model(inputs=[input], outputs=[output])\n",
    "\n",
    "    return model\n",
    "\n",
    "def DNN(num_classes, multi_label_classifier=False):\n",
    "    activation = keras.activations.sigmoid if multi_label_classifier else keras.activations.softmax\n",
    "\n",
    "    def Conv_block_1(filters, kernel_size, pool_size):\n",
    "        return keras.Sequential([\n",
    "            layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Dropout(rate=0.1),\n",
    "            layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same'),\n",
    "            layers.MaxPooling1D(pool_size=pool_size)\n",
    "        ]), layers.MaxPooling1D(pool_size=pool_size)\n",
    "\n",
    "    def Conv_block_2(filters, kernel_size, pool_size):\n",
    "        return keras.Sequential([\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Dropout(rate=0.1),\n",
    "            layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Dropout(rate=0.1),\n",
    "            layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='same'),\n",
    "            layers.MaxPooling1D(pool_size=pool_size)\n",
    "        ]), layers.MaxPooling1D(pool_size=pool_size)\n",
    "\n",
    "    shape = (1000, 12)\n",
    "    input = keras.Input(shape=shape, name=\"ECG\")\n",
    "\n",
    "    x = keras.Sequential([\n",
    "        layers.Conv1D(filters=32, kernel_size=16, padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU()\n",
    "    ])(input)\n",
    "\n",
    "    x1, x2 = Conv_block_1(kernel_size=16, filters=32, pool_size=2)\n",
    "    x = layers.Concatenate()([x1(x), x2(x)])\n",
    "\n",
    "    for filters, kernel_size, pool_size in zip(\n",
    "        [32, 32, 32, 64, 64, 64, 64, 96, 96, 96, 96],\n",
    "        [16, 16, 16, 16, 16,  8,  8,  8,  8,  8,  8],\n",
    "        [ 1,  2,  1,  2,  1,  2,  1,  2,  1,  2,  1]\n",
    "    ):\n",
    "        x1, x2 = Conv_block_2(filters=filters, kernel_size=kernel_size, pool_size=pool_size)\n",
    "        x = layers.Concatenate()([x1(x), x2(x)])\n",
    "\n",
    "    x = keras.Sequential([\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU()\n",
    "    ])(x)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True), merge_mode='sum')(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = layers.Dense(units=64, activation=keras.activations.relu)(x)\n",
    "    output = layers.Dense(units=num_classes, activation=activation, name='output')(x)\n",
    "\n",
    "    model = keras.Model(inputs=[input], outputs=[output])\n",
    "\n",
    "    return model\n",
    "        \n",
    "# Загрузка ptbxl\n",
    "def load_ptbxl(task):\n",
    "  if task == 'diag':\n",
    "    X_train = np.load('X_train_ptbxl_diag.npy')\n",
    "    y_train = np.load('y_train_ptbxl_diag.npy')\n",
    "    X_test = np.load('X_val_ptbxl_diag.npy')\n",
    "    y_test = np.load('y_val_ptbxl_diag.npy')\n",
    "  elif task == 'superdiag':\n",
    "    X_train = np.load('X_train_ptbxl_superdiag.npy')\n",
    "    y_train = np.load('y_train_ptbxl_superdiag.npy')\n",
    "    X_test = np.load('X_val_ptbxl_superdiag.npy')\n",
    "    y_test = np.load('y_val_ptbxl_superdiag.npy')\n",
    "  elif task == 'all':\n",
    "    X_train = np.load('data_npy/ptbxlnpy/X_train_ptbxl_all.npy')\n",
    "    y_train = np.load('data_npy/ptbxlnpy/y_train_ptbxl_all.npy')\n",
    "    X_test = np.load('data_npy/ptbxlnpy/X_val_ptbxl_all.npy')\n",
    "    y_test = np.load('data_npy/ptbxlnpy/y_val_ptbxl_all.npy')\n",
    "  elif task == 'subdiag':\n",
    "    X_train = np.load('X_train_ptbxl_subdiag.npy')\n",
    "    y_train = np.load('y_train_ptbxl_subdiag.npy')\n",
    "    X_test = np.load('X_val_ptbxl_subdiag.npy')\n",
    "    y_test = np.load('y_val_ptbxl_subdiag.npy')\n",
    "  return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Сериализация\n",
    "def sp(obj, name): # sp = serialization_pickle ;name = *.pkl\n",
    "    with open(name, 'wb') as pickle_out:\n",
    "        pickle.dump(obj, pickle_out)\n",
    "\n",
    "# Компиляция и обучение модели\n",
    "def AUC_Keras(y_true, y_pred):\n",
    "    auc = keras.metrics.AUC(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "# Компиляция и обучение модели\n",
    "def compile_fit(model, X_train, y_train, X_val = None, y_val = None, validation_split = 0.0, batch_size = None, epochs = None, early_stopping = None, model_checkpoint = None, onecycle = None):\n",
    "  model.compile(loss = keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.experimental.AdamW(),\n",
    "                metrics=['AUC'])\n",
    "  \n",
    "  if X_val == None:\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs = epochs, \n",
    "                        batch_size = batch_size,\n",
    "                        validation_data = None, \n",
    "                        validation_split=validation_split, \n",
    "                        callbacks=[onecycle, model_checkpoint, early_stopping])\n",
    "    sp(history, 'history.pkl')\n",
    "  else:\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs = epochs,\n",
    "                        batch_size = batch_size, \n",
    "                        validation_data = (X_val, y_val), \n",
    "                        validation_split=0.0, \n",
    "                        callbacks=[onecycle, model_checkpoint, early_stopping])\n",
    "    sp(history, 'history.pkl')\n",
    "  return history\n",
    "\n",
    "# TP TN FP FN\n",
    "def tp_tn_fp_fn(y_true, y_pred):\n",
    "  TP = TruePositives()\n",
    "  TN = TrueNegatives()\n",
    "  FP = FalsePositives()\n",
    "  FN = FalseNegatives()\n",
    "  TP.update_state(y_true, y_pred)\n",
    "  TN.update_state(y_true, y_pred)\n",
    "  FP.update_state(y_true, y_pred)\n",
    "  FN.update_state(y_true, y_pred)\n",
    "  return TP.result().numpy(),  TN.result().numpy(),  FP.result().numpy(), FN.result().numpy() \n",
    "\n",
    "# Подсчет метрик\n",
    "def calc_metrics(t, p, flag = 0): # t - y_true, p - y_pred\n",
    "  y_true=np.argmax(t, axis=1)\n",
    "  y_pred=np.argmax(p, axis=1)\n",
    "  beta = 2\n",
    "\n",
    "  f2_score = fbeta_score(y_true, y_pred, average='macro', beta=2)\n",
    "  precision = precision_score(y_true, y_pred, average='macro')\n",
    "  recall = recall_score(y_true, y_pred, average='macro')\n",
    "  TP, TN, FP, FN = tp_tn_fp_fn(t, p)\n",
    "  g2_score = TP/(TP+FP+beta*FN)\n",
    "    \n",
    "  sp(f2_score, 'f2_score.pkl')\n",
    "  sp(g2_score, 'g2_score.pkl')\n",
    "  sp(precision, 'precision.pkl')\n",
    "  sp(recall, 'recall.pkl')\n",
    "  sp(TP, 'TP.pkl')\n",
    "  sp(TN, 'TN.pkl')\n",
    "  sp(FP, 'FP.pkl')\n",
    "  sp(FN, 'FN.pkl')\n",
    "  sp(y_true, 'y_true_argmax.pkl')\n",
    "  sp(y_pred, 'y_pred_argmax.pkl')\n",
    "    \n",
    "  if flag == 0:\n",
    "    return f2_score, g2_score\n",
    "  elif flag == 1:\n",
    "    return f2_score, g2_score, precision, recall\n",
    "\n",
    "# Таблица результатов\n",
    "table_res_ptbxl = pd.DataFrame(columns = ('AUC', 'F2', 'G2'))\n",
    "\n",
    "# Занесение новых результатов в таблицу\n",
    "def edit_table(table, model, X, y, index_name): # X - X_test, y - y_test \n",
    "  score = model.evaluate(X, y)\n",
    "  y_pr = model.predict(X)\n",
    "  f2_score, g2_score = calc_metrics(y, y_pr, flag = 0)\n",
    "  list_metrics = [score[1], f2_score, g2_score] # AUC, F2, G2\n",
    "  table.loc[index_name] = list_metrics\n",
    "\n",
    "  sp(score, 'score.pkl')\n",
    "  sp(y_pr, 'y_pred.pkl')\n",
    "  sp(y, 'y_test.pkl')\n",
    "    \n",
    "  return table\n",
    "\n",
    "# График loss и accuracy\n",
    "def plot_loss_and_accuracy_curves(_history):\n",
    "  fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18,6))\n",
    "  axs[0].plot(_history.history['loss'], color='b', label='Training loss')\n",
    "  axs[0].plot(_history.history['val_loss'], color='r', label='Validation loss')\n",
    "  axs[0].set_title(\"Loss\")\n",
    "  axs[0].legend(loc='best', shadow=True)\n",
    "  axs[1].plot(_history.history['auc'], color='b', label='Training accuracy')\n",
    "  axs[1].plot(_history.history['val_auc'], color='r', label='Validation accuracy')\n",
    "  axs[1].set_title(\"AUC\")\n",
    "  axs[1].legend(loc='best', shadow=True)\n",
    "  plt.show()\n",
    "  \n",
    "# Работа с моделями lstm и lstm_bidir\n",
    "def type_comp_fit_save_model_score(table, X_train, y_train, X_test, y_test, type_model, save_name, index_model_task, batch_size, epochs, onecycle):\n",
    "  # количество классов\n",
    "  num_classes = y_train.shape[1]\n",
    "  sp(num_classes, 'num_classes.pkl')\n",
    "\n",
    "  # Выбор архитектуры модели\n",
    "  if type_model == 'lstm':\n",
    "    inputs = keras.Input(shape=(1000, 12))\n",
    "    x = layers.LSTM(units=256,\n",
    "                    return_sequences=True,\n",
    "                    stateful=False,\n",
    "                    unroll=False)(inputs)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.LSTM(units=256,\n",
    "                    return_sequences=True, # True if concat\n",
    "                    stateful=False,\n",
    "                    unroll=False)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "    max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "    concat = layers.Concatenate()([avg_pool, max_pool])\n",
    "    outputs = layers.Dense(units=num_classes, activation='sigmoid')(concat)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    print(model.summary())\n",
    "    \n",
    "    sp(model, 'model.pkl')\n",
    "    sp(model.summary(), 'model_summary().pkl')\n",
    "    \n",
    "    # Реализация раннего прекращения.\n",
    "    checkpoint_filepath = './checkpoint_lstm/'\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                          save_weights_only=True,\n",
    "                                                          save_best_only=True)\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=epochs//10,\n",
    "                                                  restore_best_weights=True)\n",
    "  elif type_model == 'lstm_bidir':\n",
    "    inputs = keras.Input(shape=(1000, 12))\n",
    "    x = layers.Bidirectional(layers.LSTM(units=256,\n",
    "                             return_sequences = True,\n",
    "                             stateful = False,\n",
    "                             unroll = False))(inputs)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(units=256,\n",
    "                             return_sequences = True,\n",
    "                             stateful = False,\n",
    "                             unroll = False))(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "    max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "    concat = layers.Concatenate()([avg_pool, max_pool])\n",
    "    outputs = layers.Dense(units=num_classes, activation='sigmoid')(concat)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.build(input_shape = (None, 1000, 12)) # `input_shape` is the shape of the input data\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    # Реализация раннего прекращения.\n",
    "    checkpoint_filepath = './checkpoint_lstm_bidir/'\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                          save_weights_only=True,\n",
    "                                                          save_best_only=True)\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=epochs//10,\n",
    "                                                  restore_best_weights=True)\n",
    "  \n",
    "  # Обучение\n",
    "  History = compile_fit(model, X_train, y_train, validation_split=0.1, batch_size=batch_size, epochs=epochs, early_stopping=early_stopping, model_checkpoint=model_checkpoint, onecycle=onecycle)\n",
    "\n",
    "  # Сохранение модели\n",
    "  model.save(save_name)\n",
    "\n",
    "  # Построение графика\n",
    "  plot_loss_and_accuracy_curves(History)\n",
    "\n",
    "  # Сохранение в таблицу\n",
    "  table = edit_table(table, model, X_test, y_test, index_model_task)\n",
    "\n",
    "  sp(table, 'table.pkl')\n",
    "  return table\n",
    "                        \n",
    "tf.random.set_seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b968e6",
   "metadata": {},
   "source": [
    "### Модель lstm на категории \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a6d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_ptbxl(task = 'all')\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * epochs, max_rate=0.0001)\n",
    "table_res_ptbxl = type_comp_fit_save_model_score(table_res_ptbxl, X_train, y_train, X_test, y_test,type_model = 'lstm', save_name = 'model_lstm_all',index_model_task = 'lstm_all', batch_size=batch_size, epochs=epochs, onecycle=onecycle)\n",
    "del(X_train)\n",
    "del(y_train)\n",
    "del(X_test)\n",
    "del(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac243e3f",
   "metadata": {},
   "source": [
    "### Модель lstm_bidir на категории \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777734ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_ptbxl(task = 'all')\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * epochs, max_rate=0.001)\n",
    "table_res_ptbxl = type_comp_fit_save_model_score(table_res_ptbxl, X_train, y_train, X_test, y_test,type_model = 'lstm_bidir', save_name = 'model_lstm_bidir_all',index_model_task = 'lstm_bidir_all', batch_size=batch_size, epochs=epochs, onecycle=onecycle)\n",
    "del(X_train)\n",
    "del(y_train)\n",
    "del(X_test)\n",
    "del(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f5d6e",
   "metadata": {},
   "source": [
    "### Модель ATI-CNN_ML на категории \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e564c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ATI_CNN(num_classes=5, multi_label_classifier=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=[keras.losses.BinaryCrossentropy()],\n",
    "    metrics=[keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Прекратить обучение если `val_loss` больше не улучшается\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        # \"больше не улучшается\" определим как \"не лучше чем min_delta и меньше\"\n",
    "        min_delta=1e-2,\n",
    "        # \"больше не улучшается\" далее определим как \"как минимум в течение patience эпох\"\n",
    "        patience=5,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='drive/MyDrive/sciwork/ATI_CNN_ML.h5',\n",
    "        # Путь по которому нужно сохранить модель\n",
    "        # Два параметра ниже значат что мы перезапишем\n",
    "        # текущий чекпоинт в том и только в том случае, когда\n",
    "        # улучится значение `val_loss`.\n",
    "        save_best_only=True,\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=callbacks,\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1715f",
   "metadata": {},
   "source": [
    "### Модель M_MBLF_ML на категории \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f44f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M_ATI_CNN(num_classes=5, multi_label_classifier=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=[keras.losses.BinaryCrossentropy()],\n",
    "    metrics=[keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Прекратить обучение если `val_loss` больше не улучшается\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        # \"больше не улучшается\" определим как \"не лучше чем min_delta и меньше\"\n",
    "        min_delta=1e-2,\n",
    "        # \"больше не улучшается\" далее определим как \"как минимум в течение patience эпох\"\n",
    "        patience=5,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='drive/MyDrive/sciwork/M_ATI_CNN_ML.h5',\n",
    "        # Путь по которому нужно сохранить модель\n",
    "        # Два параметра ниже значат что мы перезапишем\n",
    "        # текущий чекпоинт в том и только в том случае, когда\n",
    "        # улучится значение `val_loss`.\n",
    "        save_best_only=True,\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=callbacks,\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bcd94f",
   "metadata": {},
   "source": [
    "### Модель DNN_ML на категории \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0afee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(num_classes=5, multi_label_classifier=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=[keras.losses.BinaryCrossentropy()],\n",
    "    metrics=[keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Прекратить обучение если `val_loss` больше не улучшается\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        # \"больше не улучшается\" определим как \"не лучше чем min_delta и меньше\"\n",
    "        min_delta=1e-2,\n",
    "        # \"больше не улучшается\" далее определим как \"как минимум в течение patience эпох\"\n",
    "        patience=5,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='drive/MyDrive/sciwork/DNN_ML.h5',\n",
    "        # Путь по которому нужно сохранить модель\n",
    "        # Два параметра ниже значат что мы перезапишем\n",
    "        # текущий чекпоинт в том и только в том случае, когда\n",
    "        # улучится значение `val_loss`.\n",
    "        save_best_only=True,\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=callbacks,\n",
    "        shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
